# RunPod Serverless Configuration
# Northeastern University Chatbot

# GPU Configuration
gpu:
  type: "RTX 4090"  # Options: RTX 4090, A40, A100
  memory: "24GB"
  cuda_version: "12.1"

# Container Configuration
container:
  image: "runpod/pytorch:2.1.0-py3.10-cuda12.1.0-devel"
  env:
    - OPENAI_API_KEY: ""  # Set in RunPod dashboard
    - CHROMA_API_KEY: ""   # Set in RunPod dashboard
    - CHROMA_HOST: "api.trychroma.com"
    - CHROMA_PORT: "8000"
    - CHROMA_TENANT: "default_tenant"
    - CHROMA_DATABASE: "default_database"
    - PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:512"

# Serverless Settings
serverless:
  handler: "runpod_handler.handler"
  max_workers: 10
  min_workers: 0
  max_concurrency: 10
  idle_timeout: 30  # seconds
  execution_timeout: 120  # seconds
  
# Resource Limits
resources:
  cpu: 8
  memory: "32GB"
  disk: "20GB"

# Scaling
scaling:
  min_instances: 0
  max_instances: 10
  target_utilization: 0.7

# Networking
network:
  enable_ipv6: false
  ports:
    - 8000

# Logging
logging:
  level: "INFO"
  format: "json"

